{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObobkyPvGlmdWQRT0JjZxB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/angelogener/CorpusForEduryone/blob/main/Chemistry_Corpus_Creator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chemistry Corpus Creator\n",
        "\n",
        "Most of this code in creating the corpus has been pioneered by Jan already and I have just changed it a bit to suit what I have done. Most of the credit in this colab goes to him. Thanks to the hard work of everyone in Social Good for pushing on!\n",
        "\n",
        "Note: Some parts of my code can probably be cut down and you are very much welcome to take what has been made and optimize it!"
      ],
      "metadata": {
        "id": "wdy8CyTgzpl-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Initialize Google Collab with libraries and files\n",
        "\n",
        "The libraries we will use are:   \n",
        "\n",
        "* **PyPDF2**, (for parsing PDF's)\n",
        "* **google.colab**, (for file input)\n",
        "* **re**, or Python's built in Regular Expression library (for parsing text and filtering out unnecessary characters)\n",
        "* **nltk**, or Natural Language Toolkit (for processing human language)\n",
        "* **docx**, (for writing to docx files)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ruk2GOC1_xHs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnPGTPX2p4OO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "872bb240-9c17-4fb7-eae5-01b04ada5911"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.0-py3-none-any.whl (239 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.6/239.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.5.0)\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2\n",
        "!pip install python-docx"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As Jan briefly touches on, 'punkt', 'stopwords', 'wordnet' are just datasets used to clean up our words."
      ],
      "metadata": {
        "id": "_tin1WRSQ9kD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from PyPDF2 import PdfReader\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from docx import Document\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "regxTI1fJ6yb",
        "outputId": "5b132de5-8287-41d0-de0f-ae11233f0767"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2f2b2d0f-5989-4fcb-8067-85505a750447\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2f2b2d0f-5989-4fcb-8067-85505a750447\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Nelson-Chemistry-11-Glossary.pdf to Nelson-Chemistry-11-Glossary.pdf\n",
            "Saving Nelson-Chemistry-12_glossary_index.pdf to Nelson-Chemistry-12_glossary_index.pdf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Create Functions to Process and Create Documents\n",
        "\n",
        " What I'll be doing is collecting my corpus from relevant words, so I'll start by collecting from the Ontario Chemistry Textbook Glossaries. First we collect all text from these glossaries\n",
        "\n",
        "We will approach two different aspects:\n",
        "* Creating a function to read **any** pdf (plus omit characters < 2)\n",
        "* Process any text by omitting any unneccessary components: ***Uppercasing, Punctuation, Stopwords, and Non-Baseform Words (Adverbs etc.)***\n",
        "* Create the final corpus!\n",
        "* Create a corpus\n",
        "\n"
      ],
      "metadata": {
        "id": "shl4Nt7IKnKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Creates a list of words contained in any given PDF\n",
        "\"\"\"\n",
        "def read_pdf(pdf: str) -> str:\n",
        "    # Instantiate a new reader\n",
        "    reader = PdfReader(pdf)\n",
        "    pdf_text = ''\n",
        "\n",
        "    for page in reader.pages:\n",
        "      content = page.extract_text()\n",
        "\n",
        "      # Only append non-empty pages and w\n",
        "      if content:\n",
        "        pdf_text += content\n",
        "\n",
        "    return pdf_text\n",
        "\n",
        "\"\"\"\n",
        "Uses nltk to convert most of the words in their base form.\n",
        "We want this function to clean as many words as possible before we\n",
        "have to physically clean it (since chemistry will have various terms that\n",
        "may not exist in the nltk).\n",
        "\"\"\"\n",
        "\n",
        "def preprocess(text: str):\n",
        "    # Lowercase, remove punctuation, and tokenize\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z\\s]+', ' ', text)\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "    return lemmatized_tokens\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Using the other functions listed above, we finally create a corpus of\n",
        "relevant words.\n",
        "\"\"\"\n",
        "def create_corpus(texts: list[str]) -> list[str]:\n",
        "    corpus = []\n",
        "    for text in texts:\n",
        "        preprocessed_text = preprocess(text)\n",
        "        corpus.extend(preprocessed_text)\n",
        "    return corpus\n",
        "\n",
        "\"\"\"\n",
        "From a corpus, write to a document to be cleaned and upload\n",
        "for use for TF table calculation.\n",
        "\"\"\"\n",
        "def create_document(corpus: list[str]):\n",
        "  # Instantiate a new Document\n",
        "  document = Document()\n",
        "\n",
        "  # Format!\n",
        "  document.add_heading('Chemistry Corpus')\n",
        "\n",
        "  # Fill pages\n",
        "  corpus_words = \", \".join(corpus)\n",
        "  document.add_paragraph(corpus_words)\n",
        "\n",
        "  # Save\n",
        "  document.save('chemistry corpus.docx')\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "A function to help locate long joined words in the finalized\n",
        "corpus doc.\n",
        "\"\"\"\n",
        "def long_words(words: list[str]) -> list[str]:\n",
        "\n",
        "  to_check = []\n",
        "  for word in words:\n",
        "\n",
        "    # I will be setting 10 as the minimum word length of concern\n",
        "    # and I do not want repeating instances as I will search these up\n",
        "    if len(word) >= 10 and word not in to_check:\n",
        "      to_check.append(word)\n",
        "\n",
        "  return to_check\n",
        "\n",
        "\"\"\"\n",
        "Makes a document from words to check.\n",
        "\"\"\"\n",
        "def create_long(corpus: list[str]):\n",
        "  # Instantiate a new Document\n",
        "  document = Document()\n",
        "\n",
        "  # Format!\n",
        "  document.add_heading('Long Words')\n",
        "\n",
        "  # Fill pages\n",
        "  corpus_words = \", \".join(corpus)\n",
        "  document.add_paragraph(corpus_words)\n",
        "\n",
        "  # Save\n",
        "  document.save('long words.docx')\n",
        "\n"
      ],
      "metadata": {
        "id": "7MLEjzVTga1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Process!\n",
        "\n",
        "Put it all together to start processing our Chemistry Glossaries!\n"
      ],
      "metadata": {
        "id": "IKzXA6rGstqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read PDF's\n",
        "\n",
        "texts = []\n",
        "texts.append(read_pdf('Nelson-Chemistry-11-Glossary.pdf'))\n",
        "texts.append(read_pdf('Nelson-Chemistry-12_glossary_index.pdf'))\n",
        "\n",
        "corpus = create_corpus(texts)\n",
        "words_to_check = long_words(corpus)\n",
        "\n",
        "create_document(corpus)\n",
        "create_long(words_to_check)"
      ],
      "metadata": {
        "id": "a2chMDNlotwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF Table Calculations\n",
        "Now that the corpus have been made, we now calculate the term frequency relative to the total amount of words in corpus."
      ],
      "metadata": {
        "id": "D9K6RwUoDhEO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Import new libraries and upload processed documents\n",
        "Since we have processed our word document, we upload our document and take in any other libraries for this stage of the process.\n",
        "\n",
        "Note: Due to \"p\" being the most frequent entry, I would have to omit entries that are less than 3 characters long, these would encapsulate all the \"article\" and acronyms for some molecules and polymers."
      ],
      "metadata": {
        "id": "P3h9gY8lE6iY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "CWiXt4zDE6OA",
        "outputId": "68587d62-4465-474a-9a43-b647730b6faf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-82eeffdc-15e2-47d7-88ba-3be1cb0a6417\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-82eeffdc-15e2-47d7-88ba-3be1cb0a6417\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving chemistry corpus.docx to chemistry corpus (1).docx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Similar read_pdf but takes a .docx file instead\n",
        "\"\"\"\n",
        "def upload_doc(path: str) -> list[str]:\n",
        "  document = Document(path)\n",
        "  cleaned_words = []\n",
        "\n",
        "  # Repeat the process\n",
        "  for paragraph in document.paragraphs:\n",
        "    text = paragraph.text\n",
        "    text = re.sub(r'[^a-zA-Z\\s]+', ' ', text)\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "\n",
        "    # Filter out words less than 3 characters\n",
        "    lemmatized_tokens = [word for word in lemmatized_tokens if len(word) > 2]\n",
        "\n",
        "    cleaned_words.extend(lemmatized_tokens)\n",
        "\n",
        "  return lemmatized_tokens\n",
        "\n",
        "\"\"\"\n",
        "Makes a dictionary pointing words to their count\n",
        "\"\"\"\n",
        "def make_dict(words: list[str]) -> dict[str, int]:\n",
        "  word_freq = {}\n",
        "  for word in words:\n",
        "\n",
        "    if word in word_freq:\n",
        "      word_freq[word] += 1\n",
        "\n",
        "    else:\n",
        "      word_freq[word] = 1\n",
        "\n",
        "  return word_freq\n"
      ],
      "metadata": {
        "id": "s2fomSd8GKo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Produce Dataframe"
      ],
      "metadata": {
        "id": "MlQwuuyofaPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned = upload_doc('clean corpus.docx')\n",
        "total = len(cleaned)\n",
        "\n",
        "word_dict = make_dict(cleaned)\n",
        "\n",
        "# We turn our word_dict into a dictionary that the dataframe can use\n",
        "\n",
        "for_df = {'Words': [], 'Term Frequency': []}\n",
        "\n",
        "for word in word_dict:\n",
        "\n",
        "  # Append unique words\n",
        "  for_df['Words'].append(word)\n",
        "\n",
        "  # Append the ratio of the frequency / total words\n",
        "  # Using the f-string to format into a percent\n",
        "  to_percent = word_dict[word]/total\n",
        "  percentage = f\"{to_percent:.2%}\"\n",
        "\n",
        "  for_df['Term Frequency'].append(percentage)\n",
        "\n",
        "# Turn into Data Frame\n",
        "term_freq = pd.DataFrame(for_df)\n",
        "\n",
        "# Filer it so we can have a descending df\n",
        "freq_filered = term_freq.sort_values(by='Term Frequency', ascending=False)\n",
        "\n",
        "print(freq_filered)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgrTvfI8ferH",
        "outputId": "868644cf-13d9-445f-857d-546a9dbb5d50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               Words Term Frequency\n",
            "9           reaction          2.01%\n",
            "5               acid          1.61%\n",
            "56              atom          1.51%\n",
            "125           energy          1.23%\n",
            "8           chemical          1.13%\n",
            "...              ...            ...\n",
            "1273  electronically          0.01%\n",
            "1272      alkalinity          0.01%\n",
            "1270          linked          0.01%\n",
            "1267           alone          0.01%\n",
            "2316          zeeman          0.01%\n",
            "\n",
            "[2317 rows x 2 columns]\n"
          ]
        }
      ]
    }
  ]
}